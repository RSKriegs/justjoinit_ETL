{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f20f73-ed47-470c-a32b-67e7251ccfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run requirements before accessing notebook\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config.json\n",
    "config = json.load(open('config.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from 2022\n",
    "data_1 = pd.read_csv(config['2022_data'])\n",
    "data_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from 2023\n",
    "data_2 = pd.read_json(config['2023_data'])\n",
    "data_2['experience'] = data_2['experience'].explode()\n",
    "data_2['operating_mode'] = data_2['operating_mode'].explode()\n",
    "data_2['type_of_work'] = data_2['type_of_work'].explode()\n",
    "data_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. extract salaries horizontally across with link for each employment type option\n",
    "salaries_df = data_2[['link','salaries']]\n",
    "\n",
    "# Explode the 'salaries' column\n",
    "salaries_df = salaries_df.explode('salaries').reset_index(drop=True)\n",
    "\n",
    "# Normalize the exploded column to extract keys into separate columns\n",
    "salaries_normalized = pd.json_normalize(salaries_df['salaries'])\n",
    "\n",
    "# Strip leading and trailing whitespaces in 'employment_type'\n",
    "salaries_normalized['employment_type'] = salaries_normalized['employment_type'].str.strip()\n",
    "\n",
    "# Merge normalized salaries with the exploded DataFrame\n",
    "merged_df = pd.concat([salaries_df.drop('salaries', axis=1), salaries_normalized], axis=1)\n",
    "merged_df['lower'] = merged_df['lower'].apply(lambda x: x.replace(' ', '') if x != None else x).astype(float)\n",
    "merged_df['upper'] = merged_df['upper'].apply(lambda x: x.replace(' ', '') if x != None else x).astype(float)\n",
    "\n",
    "# Pivot the DataFrame to create separate columns for each employment type\n",
    "pivot_df = merged_df.pivot_table(\n",
    "    index='link',\n",
    "    columns='employment_type',\n",
    "    values=['lower', 'upper'],\n",
    "    aggfunc='first'\n",
    ")\n",
    "\n",
    "# Flatten the multi-level column index\n",
    "pivot_df.columns = ['_'.join(col).strip() for col in pivot_df.columns.values]\n",
    "\n",
    "# Reset index to bring 'link' back as a column\n",
    "pivot_df = pivot_df.reset_index()\n",
    "\n",
    "# Merge with the original 'link' column\n",
    "salaries_df = salaries_df[['link']].merge(pivot_df, on='link', how='left').drop_duplicates()\n",
    "\n",
    "salaries_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. extract skills to 12 columns, replacing values for \"regular\" etc\n",
    "skills_df = data_2[['link','skills']]\n",
    "\n",
    "# Mapping values to new values\n",
    "value_mapping = {\n",
    "    \"master\": 5,\n",
    "    \"advanced\": 4,\n",
    "    \"regular\": 3,\n",
    "    \"junior\": 2,\n",
    "    \"nice to have\": 1\n",
    "}\n",
    "\n",
    "# Replace values in the 'skills' column based on the mapping\n",
    "skills_df.loc[:, 'skills'] = skills_df['skills'].apply(lambda x: {k: value_mapping.get(v, v) for k, v in x.items()})\n",
    "\n",
    "# Create a function to extract keys and values\n",
    "def extract_skills(row):\n",
    "    skills_dict = row['skills']\n",
    "    skills_keys = list(skills_dict.keys())\n",
    "    skills_values = list(skills_dict.values())\n",
    "    return pd.Series(skills_keys + skills_values, index=['skills_name_' + str(i) for i in range(len(skills_keys))] +\n",
    "                                                     ['skills_value_' + str(i) for i in range(len(skills_values))])\n",
    "\n",
    "# Apply the function across the DataFrame\n",
    "skills_extracted = skills_df.apply(extract_skills, axis=1)\n",
    "\n",
    "# Concatenate the extracted skills with the 'link' column\n",
    "skills_df = pd.concat([skills_df['link'], skills_extracted], axis=1)\n",
    "\n",
    "# # Pivot the DataFrame\n",
    "# skills_result_df = skills_result_df.pivot_table(\n",
    "#     index='link',\n",
    "#     aggfunc='first'\n",
    "# )\n",
    "\n",
    "# Convert specific columns to integer type\n",
    "int_columns = [col for col in skills_df.columns if 'skills_value_' in col]\n",
    "skills_df[int_columns] = skills_df[int_columns].astype(pd.Int64Dtype())\n",
    "\n",
    "skills_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. extract a subset of data from data_1 for a last month\n",
    "data_1['Published_at'] = pd.to_datetime(data_1['Published_at'])\n",
    "\n",
    "# Find the maximum date\n",
    "max_date = data_1['Published_at'].max()\n",
    "\n",
    "# Subtract one month from the maximum date\n",
    "start_date = max_date - pd.DateOffset(days=45) #the date is arbitrary based on the estimations between initial commit and minimal date\n",
    "\n",
    "# Filter the DataFrame based on the condition\n",
    "data_1 = data_1[data_1['Published_at'] >= start_date]\n",
    "\n",
    "# Filter rows containing 'data' and 'engineer' in 'Title' column (case insensitive)\n",
    "data_1 = data_1[data_1['Title'].str.contains('data', case=False) & data_1['Title'].str.contains('engineer', case=False)]\n",
    "\n",
    "data_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. create final data_2 object\n",
    "#merge data_2\n",
    "data_2 = pd.merge(data_2, salaries_df, on='link', how='inner')\n",
    "data_2 = pd.merge(data_2, skills_df, on='link', how='inner')\n",
    "\n",
    "#drop columns\n",
    "data_2 = data_2.drop(['salaries', 'skills'], axis=1)\n",
    "\n",
    "# Filter rows containing 'data' and 'engineer' in 'name' column (case insensitive)\n",
    "data_2 = data_2[data_2['name'].str.contains('data', case=False) & data_2['name'].str.contains('engineer', case=False)]\n",
    "\n",
    "data_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. create a data model for a target data frame and append rows there\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'title'            : pd.Series(dtype=str),\n",
    "        'category'         : pd.Series(dtype=str),\n",
    "        'location'         : pd.Series(dtype=str),\n",
    "        'experience'       : pd.Series(dtype=str),\n",
    "        'operating_mode'   : pd.Series(dtype=str),\n",
    "        'salary_lower_B2B' : pd.Series(dtype=float),\n",
    "        'salary_upper_B2B' : pd.Series(dtype=float),\n",
    "        'salary_lower_UoP' : pd.Series(dtype=float),\n",
    "        'salary_upper_UoP' : pd.Series(dtype=float),\n",
    "        'skills_name_0'    : pd.Series(dtype=str),\n",
    "        'skills_value_0'   : pd.Series(dtype=int),\n",
    "        'skills_name_1'    : pd.Series(dtype=str),\n",
    "        'skills_value_1'   : pd.Series(dtype=int),\n",
    "        'skills_name_2'    : pd.Series(dtype=str),\n",
    "        'skills_value_2'   : pd.Series(dtype=int),\n",
    "        'year'             : pd.Series(dtype=int)\n",
    "    },\n",
    "    index=pd.Index(range(0), name='id'))\n",
    "\n",
    "data_1 = data_1[['Title','Marker_icon','City','Experience_level','Workplace_type','salary_from_b2b','salary_to_b2b',\n",
    "                 'salary_from_permanent', 'salary_to_permanent', 'skills_name_0', 'skills_value_0', 'skills_name_1',\n",
    "                 'skills_value_1', 'skills_name_2', 'skills_value_2']]\n",
    "data_1['year'] = 2022\n",
    "data_1.columns = df.columns\n",
    "data_1.replace(0, np.nan, inplace=True)\n",
    "\n",
    "df = df._append(data_1)\n",
    "\n",
    "data_2 = data_2[['name','category','location','experience','operating_mode','lower_B2B','upper_B2B',\n",
    "                 'lower_Permanent', 'upper_Permanent', 'skills_name_0', 'skills_value_0', 'skills_name_1',\n",
    "                 'skills_value_1', 'skills_name_2', 'skills_value_2']]\n",
    "data_2['year'] = 2023\n",
    "data_2.columns = df.columns\n",
    "\n",
    "df = df._append(data_2)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get info about dataframe\n",
    "df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
